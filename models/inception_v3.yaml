---
dataset:
    task:
        background_class: {use: true}
        preprocess:
            shape:
                height: 299
                width: 299
                channels: 3
            validate: {type: central_crop, fraction: 0.875}
            final_cpu:
                - {type: linear_map, scale: 2.0, shift: -1.0}
                - {type: resize, fill: false}
model:
    name: inception_v3
    description: |
        This implementation is based on::
          Rethinking the Inception Architecture for Computer Vision
          https://arxiv.org/abs/1512.00567
        We use the following reference models::
          https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception_v3.py
    layers:
        _conv: &conv_valid
            type: convolution
            kernel_size: 3
            stride: 1
            padding: valid
            normalizer_fn:
                tensorflow.contrib.slim.batch_norm
            normalizer_params:
                decay: 0.9997
                epsilon: 0.001
            weights_initializer:
                type: tensorflow.contrib.layers.variance_scaling_initializer
            weights_regularizer:
                type: tensorflow.contrib.layers.l2_regularizer
                scale: 0.00004
        _conv_pad: &conv_same {<<: *conv_valid, padding: same}
        # 299 x 299 x 3
        conv1: {<<: *conv_valid, stride: 2, num_outputs: 32}
        # 149 x 149 x 32
        conv2a: {<<: *conv_valid, num_outputs: 32}
        # 147 x 147 x 32
        conv2b: {<<: *conv_valid, padding: same, num_outputs: 64}
        # 147 x 147 x 64
        pool3a: &max_pool
            {type: max_pool, kernel_size: 3, stride: 2, padding: valid}
        # 73 x 73 x 64
        conv3b: {<<: *conv_valid, kernel_size: 1, num_outputs: 80}
        # 73 x 73 x 80
        conv4: {<<: *conv_valid, num_outputs: 192}
        # 71 x 71 x 192
        pool5: *max_pool
        # 35 x 35 x 192
        mixed5b: &mixed5b
            type: module
            kwargs: {d_depth: 32}
            layers:
                a1: {<<: *conv_same, kernel_size: 1, num_outputs: 64}
                b1: {<<: *conv_same, kernel_size: 1, num_outputs: 48}
                b2: {<<: *conv_same, kernel_size: 5, num_outputs: 64}
                c1: {<<: *conv_same, kernel_size: 1, num_outputs: 64}
                c2: {<<: *conv_same, kernel_size: 3, num_outputs: 96}
                c3: {<<: *conv_same, kernel_size: 3, num_outputs: 96}
                d1: &mixed_pool
                    type: average_pool
                    kernel_size: 3
                    stride: 1
                    padding: same
                d2: {<<: *conv_same, kernel_size: 1, num_outputs: ^(d_depth)}
                concat: &concat {type: concat, axis: 3}
            graph:
                - {from: input, with: a1, to: a}
                - {from: input, with: [b1, b2], to: b}
                - {from: input, with: [c1, c2, c3], to: c}
                - {from: input, with: [d1, d2], to: d}
                - {from: [a, b, c, d], with: concat, to: output}
        # 35 x 35 x 256
        mixed5c: {<<: *mixed5b, d_depth: 64}
        # 35 x 35 x 288
        mixed5d: {<<: *mixed5b, d_depth: 64}
        # 35 x 35 x 288
        mixed6a: &mixed6a
            type: module
            layers:
                a1: {<<: *conv_valid, kernel_size: 3, stride: 2,
                     num_outputs: 384}
                b1: {<<: *conv_same, kernel_size: 1, num_outputs: 64}
                b2: {<<: *conv_same, kernel_size: 3, num_outputs: 96}
                b3: {<<: *conv_valid, kernel_size: 3, stride: 2,
                     num_outputs: 96}
                c1: {type: max_pool, kernel_size: 3, stride: 2, padding: valid}
                concat: *concat
            graph:
                - {from: input, with: a1, to: a}
                - {from: input, with: [b1, b2, b3], to: b}
                - {from: input, with: c1, to: c}
                - {from: [a, b, c], with: concat, to: output}
        # 17 x 17 x 768
        mixed6b: &mixed6b
            type: module
            kwargs: {depth: 128, final_depth: 192}
            layers:
                _in: &m6_conv
                    {<<: *conv_same, kernel_size: 1, num_outputs: ^(depth)}
                _c1x7: &c1x7 {<<: *m6_conv, kernel_size: [1, 7]}
                _c7x1: &c7x1 {<<: *m6_conv, kernel_size: [7, 1]}
                a1: {<<: *m6_conv, num_outputs: ^(final_depth)}
                b1: {<<: *conv_same, kernel_size: 1, num_outputs: ^(depth)}
                b2: *c1x7
                b3: {<<: *c7x1, num_outputs: ^(final_depth)}
                c1: *m6_conv
                c2: *c7x1
                c3: *c1x7
                c4: *c7x1
                c5: {<<: *c1x7, num_outputs: ^(final_depth)}
                d1: *mixed_pool
                d2: {<<: *conv_same, kernel_size: 1,
                     num_outputs: ^(final_depth)}
                concat: *concat
            graph:
                - {from: input, with: a1, to: a}
                - {from: input, with: [b1, b2, b3], to: b}
                - {from: input, with: [c1, c2, c3, c4, c5], to: c}
                - {from: input, with: [d1, d2], to: d}
                - {from: [a, b, c, d], with: concat, to: output}
        # 17 x 17 x 768
        mixed6c: {<<: *mixed6b, depth: 160}
        # 17 x 17 x 768
        mixed6d: {<<: *mixed6b, depth: 160}
        # 17 x 17 x 768
        mixed6e: {<<: *mixed6b, depth: 192}
        # 17 x 17 x 768
        mixed7a:
            type: module
            layers:
                a1: {<<: *conv_same, kernel_size: 1, num_outputs: 192}
                a2: &m7a_conv_valid
                    {<<: *conv_valid, kernel_size: 3, stride: 2,
                     num_outputs: 320}
                b1: {<<: *conv_same, kernel_size: 1, num_outputs: 192}
                b2: {<<: *conv_same, kernel_size: [1, 7], num_outputs: 192}
                b3: {<<: *conv_same, kernel_size: [7, 1], num_outputs: 192}
                b4: {<<: *m7a_conv_valid, num_outputs: 192}
                c1: {type: max_pool, kernel_size: 3, stride: 2, padding: valid}
                concat: *concat
            graph:
                - {from: input, with: [a1, a2], to: a}
                - {from: input, with: [b1, b2, b3, b4], to: b}
                - {from: input, with: [c1], to: c}
                - {from: [a, b, c], with: concat, to: output}
        # 8 x 8 x 1280
        mixed7b: &mixed7b
            type: module
            layers:
                a1: {<<: *conv_same, kernel_size: 1, num_outputs: 320}
                b1: {<<: *conv_same, kernel_size: 1, num_outputs: 384}
                b2a: &m7b_a
                    {<<: *conv_same, kernel_size: [1, 3], num_outputs: 384}
                b2b: &m7b_b
                    {<<: *conv_same, kernel_size: [3, 1], num_outputs: 384}
                b3: *concat
                c1: {<<: *conv_same, kernel_size: 1, num_outputs: 448}
                c2: {<<: *conv_same, kernel_size: 3, num_outputs: 384}
                c3a: *m7b_a
                c3b: *m7b_b
                c4: *concat
                d1: *mixed_pool
                d2: {<<: *conv_same, kernel_size: 1, num_outputs: 192}
                concat: *concat
            graph:
                - {from: input, with: a1, to: a}
                - {from: input, with: b1, to: b1}
                - {from: b1, with: b2a, to: b2a}
                - {from: b1, with: b2b, to: b2b}
                - {from: [b2a, b2b], with: b3, to: b}
                - {from: input, with: [c1, c2], to: c2}
                - {from: c2, with: c3a, to: c3a}
                - {from: c2, with: c3b, to: c3b}
                - {from: [c3a, c3b], with: c4, to: c}
                - {from: input, with: [d1, d2], to: d}
                - {from: [a, b, c, d], with: concat, to: output}
        # 8 x 8 x 2048
        mixed7c: *mixed7b
        pool: {type: average_pool, kernel_size: 8, padding: valid}
        dropout: {type: dropout, keep_prob: 0.8}
        fc:
            <<: *conv_same
            kernel_size: 1
            num_outputs: $(dataset.task.num_classes)
            activation_fn: null
            normalizer_fn: null
        squeeze: {type: squeeze, axis: [1, 2]}
    graph:
        from: input
        with:
            [conv1, conv2a, conv2b, pool3a, conv3b, conv4, pool5,
             mixed5b, mixed5c, mixed5d,
             mixed6a, mixed6b, mixed6c, mixed6d, mixed6e,
             mixed7a, mixed7b, mixed7c,
             pool, dropout, fc, squeeze]
        to: output
